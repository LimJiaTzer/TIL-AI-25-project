{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4219745-bb4e-4706-9c66-127cc5b33e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b040429466d449afa9d9373372d42cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62617a25a14d418387bf9660ea4c00d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GotOcr2ForConditionalGeneration # You might need to find the exact import name if this is custom\n",
    "\n",
    "# Attempt to load using the specific model class\n",
    "try:\n",
    "    model = GotOcr2ForConditionalGeneration.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n",
    "except ImportError:\n",
    "    print(\"Could not import GotOcr2ForConditionalGeneration. This class might not be available in your transformers version or requires a custom script from the model provider.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading with GotOcr2ForConditionalGeneration: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "356bdbc6-4ab4-4957-9551-b1df960e3622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18fe087-046e-469c-a58a-5689db2f757c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor saved to: ./GOT-OCR-2.0-hf\n"
     ]
    }
   ],
   "source": [
    "# Save directory (relative to notebook location)\n",
    "save_path = \"./GOT-OCR-2.0-hf\"  # Creates a folder in the same directory as the notebook\n",
    "\n",
    "# Save both model and processor\n",
    "model.save_pretrained(save_path)\n",
    "processor.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and processor saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3760ef4-9cb4-4e1e-881c-fe5427b04cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6a5e057461419da9424113af4abc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6725bceb2a3c46c0aba0a6eb8dce791e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d8390c990c4be3bc20ba114bd4f708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GOT-OCR2 to ./GOT-OCR-2.0-hf\n"
     ]
    }
   ],
   "source": [
    "# OR SNAPSHOT DOWNLOAD ALL\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# this will grab everything in the model repo (including the custom .py files)\n",
    "snapshot_download(\n",
    "    repo_id=\"stepfun-ai/GOT-OCR-2.0-hf\",\n",
    "    repo_type=\"model\",\n",
    "    local_dir=\"./GOT-OCR-2-hf\"\n",
    ")\n",
    "print(\"Saved GOT-OCR2 to ./GOT-OCR-2.0-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46424368-eef6-44a5-b90e-ddd58db00dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting OCR Test ---\n",
      "Using image (absolute path): /home/jupyter/novice/ocr/sample_0.jpg\n",
      "Using model (relative path/HF ID): ./GOT-OCR-2-hf\n",
      "Attempting to load image from: /home/jupyter/novice/ocr/sample_0.jpg\n",
      "Image loaded successfully.\n",
      "\n",
      "Attempting to load processor from: ./GOT-OCR-2-hf\n",
      "Processor loaded successfully.\n",
      "\n",
      "Attempting to load model: ./GOT-OCR-2-hf\n",
      "Model loaded successfully to cpu using GotOcr2ForConditionalGeneration.\n",
      "\n",
      "Preparing inputs for the model...\n",
      "Inputs prepared.\n",
      "\n",
      "Generating text (performing OCR)...\n",
      "Text generation complete.\n",
      "\n",
      "Decoding generated text...\n",
      "--- Recognized Text ---\n",
      "TOP SECRET: Operation Iron Claw Report on Potential BH- 2000 Hideouts Analysis indicates potential BH- 2000 hideouts maybe located in remote areas identified as high- risk zones based on current geo spatial data. CYPHER' s advanced algorithms have pinpointed several areas with elevated probability of BH- 2000 activity, requiring immediate surveillance and reconnaissance efforts.  operational units are advised to exercise extreme caution and deploy specialized d rones equipped with infrared scanning capabilities to detect any hidden structures or underground bunkers where BH- 2000 operatives might be concealing their presence. Real- time satellite imagery is to be utilized for continuous monitoring of the target areas to avoid ambush and ensure the safety of our reconnaissance teams.  Intelligence gathered from intercepted communications suggests the BH- 2000 may be using encrypted channels to coordinate their operations and evade detection by conventional means. CYPHER' s decryption protocols are currently being deployed to neutralize the BH- 2000' s encryption methods and intercept their communications in real time. Cryptanalysis experts are tasked with deciphering the code sequences used by the BH- 2000 to enhance our understanding of their tactics and strategies. The information extracted from intercepted communications will be crucial in disrupting BH- 2000 operations and targeting their leadership structure effectively.  Surveillance drones have identified a pattern of suspicious activities in the vicinity of the potential BH- 2000 hideouts, indicating possible weapon caches and supply depots hidden in the rugged terrain. CYPHER' s predictive analysis algorithms have identified possible routes used by BH- 2000 supply convoys to supply their operatives in the field. Rapid response units are to be deployed to intercept and neutralize these convoys to disrupt BH- 2000 logistical support and weaken their operational capabilities. Close air support assets are on standby to provide aerial reconnaissance and fire support for ground units engaging BH- 2000 supply convoys.  Reports from field operatives suggest increased BH- 2000 activity in the vicinity of the potential hideouts, with intelligence indicating possible preparations for a large- scale offensive against friendly forces in the region. CYPHER' s threat assessment models have been updated to reflect the heightened risk of a BH- 2000 attack on our positions. Defensive measures are to be reinforced, and all units in the area are to be placed on high alert status. Rapid reaction forces are to be prepositioned at\n",
      "-----------------------\n",
      "\n",
      "--- Script Finished ---\n",
      "\n",
      "To run this script, you need to install: pip install Pillow transformers torch torchvision\n",
      "If downloading models from Hugging Face Hub, you might need to log in: `huggingface-cli login`\n",
      "Your other files (ocr.jsonl, sample_0_test.txt, sample_0.hocr) are not used in this basic inference script but are noted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "# Ensure you have transformers, torch, and Pillow installed:\n",
    "# pip install transformers torch torchvision Pillow\n",
    "# For transformers version 4.37.0, some newer features or models might require trust_remote_code=True\n",
    "try:\n",
    "    from transformers import AutoProcessor, GotOcr2ForConditionalGeneration\n",
    "except ImportError:\n",
    "    print(\"Failed to import from transformers. Ensure it's installed correctly.\")\n",
    "    print(\"You might be missing GotOcr2ForConditionalGeneration if your transformers version is too old and doesn't support it even with remote code.\")\n",
    "    exit()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Absolute path to the image\n",
    "# This should point to your actual 'sample_0.jpg'\n",
    "IMAGE_PATH_ABSOLUTE = \"/home/jupyter/novice/ocr/sample_0.jpg\"\n",
    "\n",
    "# Relative path to the model directory (or Hugging Face model identifier)\n",
    "# Option 1: If you have the model downloaded locally in a folder (e.g., \"./name_of_model\")\n",
    "# relative to where you run this script. This folder should contain config.json, model weights, etc.\n",
    "MODEL_PATH_RELATIVE = \"./GOT-OCR-2-hf\"\n",
    "\n",
    "# Option 2: Replace with a Hugging Face model identifier if you want to download/use it directly.\n",
    "# MODEL_ID_HF = \"stepfun-ai/GOT-OCR-2.0-hf\"\n",
    "# model_to_load = MODEL_ID_HF # Use this if you prefer the HF identifier\n",
    "\n",
    "# For this script, we'll use MODEL_PATH_RELATIVE and handle if it's a placeholder.\n",
    "model_to_load = MODEL_PATH_RELATIVE\n",
    "\n",
    "\n",
    "def perform_ocr(image_path, model_name_or_path):\n",
    "    \"\"\"\n",
    "    Performs OCR on a single image using the specified model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Absolute or relative path to the image file.\n",
    "        model_name_or_path (str): Path to the local model directory or Hugging Face model identifier.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load image from: {image_path}\")\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image not found at {image_path}\")\n",
    "        print(\"Please ensure the image path is correct or a dummy image can be created.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        print(\"Image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAttempting to load processor from: {model_name_or_path}\")\n",
    "    try:\n",
    "        # trust_remote_code=True might be necessary if the model includes custom code.\n",
    "        processor = AutoProcessor.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        print(\"Processor loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading processor: {e}\")\n",
    "        print(\"Hints: \")\n",
    "        print(\"- Ensure the model path/identifier is correct.\")\n",
    "        print(\"- If using a local path, it should contain 'preprocessor_config.json' and other necessary files.\")\n",
    "        print(\"- You might need to be logged in to Hugging Face CLI ('huggingface-cli login') if downloading.\")\n",
    "        print(\"- For older transformers versions, 'trust_remote_code=True' can be crucial for models with custom code.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAttempting to load model: {model_name_or_path}\")\n",
    "    device = \"cpu\" # Default to CPU; use \"cuda\" if torch.cuda.is_available() and you have a GPU\n",
    "    # import torch # Uncomment if you want to explicitly manage device with torch.cuda\n",
    "    # if torch.cuda.is_available():\n",
    "    #     device = \"cuda\"\n",
    "\n",
    "    try:\n",
    "        # Using the specific model class 'GotOcr2ForConditionalGeneration'\n",
    "        # This is often more reliable for specific architectures, especially with older transformers.\n",
    "        model = GotOcr2ForConditionalGeneration.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "        model.to(device) # Move model to the device (CPU or GPU)\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        print(f\"Model loaded successfully to {device} using GotOcr2ForConditionalGeneration.\")\n",
    "    except ImportError:\n",
    "        print(\"Error: GotOcr2ForConditionalGeneration class not found.\")\n",
    "        print(\"This specific class might not be available in your transformers version (e.g., 4.37.0) without 'trust_remote_code=True' or if the model's custom code isn't fetched/registered.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model with GotOcr2ForConditionalGeneration: {e}\")\n",
    "        print(\"Hints: \")\n",
    "        print(\"- Double-check the model path/identifier.\")\n",
    "        print(\"- If local, ensure the directory contains all model files (config.json, pytorch_model.bin, etc.).\")\n",
    "        print(\"- Compatibility issues with transformers version 4.37.0 can occur for newer models.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nPreparing inputs for the model...\")\n",
    "    try:\n",
    "        # The processor prepares the image and any necessary text prompts (though for basic OCR, text is minimal)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        print(\"Inputs prepared.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing inputs with processor: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nGenerating text (performing OCR)...\")\n",
    "    try:\n",
    "        # Prepare arguments for model.generate\n",
    "        # The `tokenizer` and `max_new_tokens` are common.\n",
    "        # `stop_strings` might be specific to models with custom generation logic (via trust_remote_code=True)\n",
    "        # In transformers 4.37.0, `stop_strings` is not a standard `model.generate` parameter.\n",
    "        # It will be attempted and caught if it causes a TypeError.\n",
    "        generate_kwargs = {\n",
    "            \"pixel_values\": inputs.get(\"pixel_values\"),\n",
    "            \"input_ids\": inputs.get(\"input_ids\"), # If processor adds BOS tokens or similar\n",
    "            \"tokenizer\": processor.tokenizer,\n",
    "            \"max_new_tokens\": 512, # Adjust as needed\n",
    "            \"num_beams\": 3 # Example: using beam search\n",
    "        }\n",
    "        \n",
    "        generated_ids = None\n",
    "        try:\n",
    "            # Some models like GOT-OCR-2.0 might accept 'stop_strings' if custom code is loaded\n",
    "            generated_ids = model.generate(**generate_kwargs, stop_strings=[\"<|im_end|>\"])\n",
    "        except TypeError as te:\n",
    "            if \"stop_strings\" in str(te).lower():\n",
    "                print(\"Warning: 'stop_strings' parameter caused a TypeError. Retrying generate without it.\")\n",
    "                print(\"         (This is common if the model's generate method or your transformers version doesn't support it directly).\")\n",
    "                # Remove the problematic kwarg and try again\n",
    "                generate_kwargs_no_stop = {k: v for k, v in generate_kwargs.items()}\n",
    "                generated_ids = model.generate(**generate_kwargs_no_stop)\n",
    "            else:\n",
    "                raise te # Re-raise if it's a different TypeError\n",
    "\n",
    "        if generated_ids is None:\n",
    "             print(\"Error: Text generation failed to produce output.\")\n",
    "             return\n",
    "        print(\"Text generation complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model.generate: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nDecoding generated text...\")\n",
    "    try:\n",
    "        recognized_text = \"\"\n",
    "        # Decoding logic based on Hugging Face examples for image-to-text models:\n",
    "        # It skips the input_ids part if they were part of the prompt.\n",
    "        if inputs.get(\"input_ids\") is not None:\n",
    "            start_index = inputs[\"input_ids\"].shape[1]\n",
    "            if generated_ids.shape[1] > start_index:\n",
    "                recognized_text = processor.decode(generated_ids[0, start_index:], skip_special_tokens=True)\n",
    "            else: # If generated sequence is not longer than input, decode the whole thing\n",
    "                recognized_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        else: # If no input_ids were in the initial processor output\n",
    "            recognized_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(\"--- Recognized Text ---\")\n",
    "        print(recognized_text)\n",
    "        print(\"-----------------------\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding text: {e}\")\n",
    "        return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Setup: Create dummy image if it doesn't exist ---\n",
    "    # This helps make the script runnable for testing purposes.\n",
    "    # In a real scenario, IMAGE_PATH_ABSOLUTE would point to your actual image.\n",
    "    dummy_ocr_dir = os.path.dirname(IMAGE_PATH_ABSOLUTE)\n",
    "    if not os.path.exists(dummy_ocr_dir):\n",
    "        try:\n",
    "            os.makedirs(dummy_ocr_dir)\n",
    "            print(f\"Created dummy directory for image: {dummy_ocr_dir}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error creating dummy directory {dummy_ocr_dir}: {e}\")\n",
    "\n",
    "    if not os.path.exists(IMAGE_PATH_ABSOLUTE):\n",
    "        try:\n",
    "            print(f\"Attempting to create a dummy image at: {IMAGE_PATH_ABSOLUTE}\")\n",
    "            img = Image.new('RGB', (600, 150), color = (220, 220, 220))\n",
    "            d = ImageDraw.Draw(img)\n",
    "            try:\n",
    "                # Try to use Arial, fallback to default if not found\n",
    "                font = ImageFont.truetype(\"arial.ttf\", 30)\n",
    "            except IOError:\n",
    "                font = ImageFont.load_default()\n",
    "            d.text((20,20), \"Sample Text for OCR Test\\n12345 ABCDE\", fill=(0,0,0), font=font)\n",
    "            img.save(IMAGE_PATH_ABSOLUTE)\n",
    "            print(f\"Created dummy image: {IMAGE_PATH_ABSOLUTE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create dummy image (PIL/font issue): {e}\")\n",
    "            print(f\"Please ensure an image exists at '{IMAGE_PATH_ABSOLUTE}' or that PIL can create one.\")\n",
    "\n",
    "    # --- Model Path Handling ---\n",
    "    # If the relative model path is the placeholder and doesn't exist,\n",
    "    # default to a Hugging Face identifier to make the example runnable.\n",
    "    current_model_to_load = model_to_load\n",
    "    if model_to_load == \"./name_of_model\" and not os.path.isdir(model_to_load):\n",
    "        print(f\"\\nWarning: The specified relative model path '{model_to_load}' does not exist or is not a directory.\")\n",
    "        print(\"This script expects it to be a local directory containing model files if not a Hugging Face identifier.\")\n",
    "        print(\"For a runnable example, defaulting to 'stepfun-ai/GOT-OCR-2.0-hf' from Hugging Face Hub.\")\n",
    "        print(\"If you have a local model at './name_of_model', ensure the path is correct and the directory exists.\")\n",
    "        current_model_to_load = \"stepfun-ai/GOT-OCR-2.0-hf\" # Default to HF model ID\n",
    "\n",
    "    # --- Run OCR ---\n",
    "    print(f\"\\n--- Starting OCR Test ---\")\n",
    "    print(f\"Using image (absolute path): {IMAGE_PATH_ABSOLUTE}\")\n",
    "    print(f\"Using model (relative path/HF ID): {current_model_to_load}\")\n",
    "    \n",
    "    perform_ocr(image_path=IMAGE_PATH_ABSOLUTE, model_name_or_path=current_model_to_load)\n",
    "\n",
    "    print(\"\\n--- Script Finished ---\")\n",
    "    print(\"\\nTo run this script, you need to install: pip install Pillow transformers torch torchvision\")\n",
    "    print(\"If downloading models from Hugging Face Hub, you might need to log in: `huggingface-cli login`\")\n",
    "    print(\"Your other files (ocr.jsonl, sample_0_test.txt, sample_0.hocr) are not used in this basic inference script but are noted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b163fd-d845-4b07-a60d-f90ea8184054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
